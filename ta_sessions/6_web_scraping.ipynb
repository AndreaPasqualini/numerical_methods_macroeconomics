{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "federal-compatibility",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stock-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import requests                 # HTTP programming\n",
    "from bs4 import BeautifulSoup   # HTML parsing\n",
    "from selenium import webdriver  # Browser automation\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "basic-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_xkcd_comic = 2436\n",
    "oldest_xkcd_comic = 2350"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-fence",
   "metadata": {},
   "source": [
    "[Web scraping](https://en.wikipedia.org/wiki/Web_scraping) is a recently popular set of techniques to create a dataset from unstructured data we can find on the Internet. There are some compelling reasons for which Economists may want to scrape data on the web. Here are some examples.\n",
    "\n",
    "- The availability of structured data is limited by the incentives to create it. Assembling a dataset is quite costly. Not only you need manual labor to enter data in the dataset, you also need to put in place mechanisms to verify that the data is accurate. Many times raw data points are not comparable, and hence require some methodology to ensure comparability. The provider of the data is also held responsible for inaccuracies or mistakes. On the other hand, the benefits are typically not enjoyed by private businesses. Often times, government agencies or comparable institutions provide public structured data. They do so because they believe there is a common, public benefit, such as academic or policy research.\n",
    "- The Internet is a medium of information exchange. Consider firms such as Amazon or eBay, who use the web to sell products and secure revenue streams. Consumers demand such services and firms supply them. This requires an exchange of information that includes, but is not limited to, quantities and prices. These firms have no incentive to release _public_ structured data about information that is exchanged on their websites. Rather, they tend to protect such data in the name of industrial interests. On the other hand, consumers have no incentive to collect data in a structured manner.\n",
    "- The Internet is also a platform for user-generated content. This characteristic of the web rose with the popularity of platforms such as Facebook, Twitter, YouTube, Reddit and so on. A researcher may be interested in collecting user-generated information (e.g., political sentiment on Twitter) for their own data analyses. Again, neither firms nor consumers have incentives in assembling structured datasets and make them public.\n",
    "\n",
    "These examples have to be considered together with the incentive every researcher has in uncovering new evidence. Some papers that are published ask old questions and use old methodologies, but provide novel answers simply because they obtained novel data. While a new dataset per se will not grant a publication per se, the implications of potentially new evidence may.\n",
    "\n",
    "In this TA class, I will show brief examples for three web scraping techniques.\n",
    "\n",
    "1. HTTP programming\n",
    "2. HTML parsing\n",
    "3. DOM parsing\n",
    "\n",
    "These are basic techniques that cover a wide set of needs. Before I explain how these techniques work, I need to provide an overview of the fundamentals of the World Wide Web.\n",
    "\n",
    "The most familiar action everybody probably takes everyday is to open a web browser, type a URL in the address bar and press the <kbd>Enter</kbd> key. When that happens, there is an exchange of information between your computer, which is called a _client_, and a remote _server_. While this exchange of information can be very complicated (e.g., it can be encrypted, or routed through other in-between servers), its most basic form involves a [HTTP request](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) and a [HTTP response](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol). When we type a URL in the address bar and press Enter, the client sends a request: it contacts the remote server and requests specific data. Upon receipt of the request, the server analyzes it, carries out the required tasks and sends back a response.\n",
    "\n",
    "The HTTP request contains the following ingredients:\n",
    "\n",
    "- A request line\n",
    "- Some request headers\n",
    "- An empty line\n",
    "- (Optional) A message body\n",
    "\n",
    "On the other hand, the HTTP response contains the following ones:\n",
    "\n",
    "- A response status line\n",
    "- Some response headers\n",
    "- An empty line\n",
    "- (Optional) A message body\n",
    "\n",
    "An example of such basic exchange is the following.\n",
    "\n",
    "The request may look like this\n",
    "\n",
    "<pre>\n",
    "GET / HTTP/1.1\n",
    "Host: www.example.com\n",
    "\n",
    "</pre>\n",
    "\n",
    "(note the empty line)\n",
    "The first line uses the method `GET` to send a request through the protocol HTTP 1.1. The request asks for all that is found at the root (i.e., `/`) of the server. The server is hosted at the address `www.example.com`. Finally, there is the mandatory empty line and no message body.\n",
    "\n",
    "The response, instead, may look like this\n",
    "\n",
    "```html\n",
    "HTTP/1.1 200 OK\n",
    "Date: Mon, 23 May 2005 22:38:34 GMT\n",
    "Content-Type: text/html; charset=UTF-8\n",
    "Content-Length: 155\n",
    "Last-Modified: Wed, 08 Jan 2003 23:11:55 GMT\n",
    "Server: Apache/1.3.3.7 (Unix) (Red-Hat/Linux)\n",
    "ETag: \"3f80f-1b6-3e1cb03b\"\n",
    "Accept-Ranges: bytes\n",
    "Connection: close\n",
    "\n",
    "<html>\n",
    "<head>\n",
    "    <title>An Example Page</title>\n",
    "</head>\n",
    "<body>\n",
    "    <p>Hello World, this is a HTML document.</p>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Here, the response acknowledges the HTTP 1.1 protocol and informs the client about the success of the request, both with the [status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) `200` and the human-readable message `OK`. The response attaches several \"metadata\" with headers, such as the date and time at which the request was served, information about the message body of the response, identification information for the server, and so on. We find the mandatory empty line and the message body, which here consists of a simple HTML file.\n",
    "\n",
    "The reason for which we need to know about HTTP requests and responses is that the three techniques I will show are adequate depending on the message body of the response.\n",
    "\n",
    "1. HTTP programming: the message body is _per se_ the data we are after. For example, it can be a CSV file, a JSON file, and so on.\n",
    "2. HTML parsing: the message body is a HTML file which contains the data we are after. This requires us to know how to navigate (programmatically) the content of the HTML file.\n",
    "3. Browser automation: the HTML code in the message body changes (through other requests) depending on what the user does. This is typically the case when the message body of the response contains JavaScript code or other code that requests additional information (e.g., think of what happens when you continuously scroll down on your Twitter feed: new tweets are dynamically loaded).\n",
    "\n",
    "Even though HTTP programming is the technique that is least likely associated with web scraping (that would imply that the data is already somewhat structured), knowing its basics is necessary because it is the starting point for the other two techniques.\n",
    "\n",
    "I will now proceed and provide one example for each of the three techniques. The operating example is the [xkcd](https://xkcd.com) website. While this example is not interesting from an economic point of view, it is quite excellent (in my opinion) for pedagogical purposes. This website (for those who do not know it) presents a set of comics drawn by Randall Munroe, a physicist. On top of being a source of fun, the website allows me to showcase how each of the three techniques listed above can work. At https://xkcd.com/about/, we even find mention of a machine-readable interface through [JSON files](https://en.wikipedia.org/wiki/JSON)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-boulder",
   "metadata": {},
   "source": [
    "## HTTP programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-merchandise",
   "metadata": {},
   "source": [
    "The Python package [requests](https://requests.readthedocs.io/) provides a simple interface and some convenience utilities to manage HTTP requests and responses. The package is quite powerful, as it delivers quite some flexibility and allows for very complicated scenarios (e.g., connections that remain open, programmatic user authentication). Here I just want to scratch the surface.\n",
    "\n",
    "XKCD's Randall Munroe is kind enough to provide us with his comics through a JSON interface. To see it, simply see https://xkcd.com/info.0.json. This is all the data we need about his latest comic. For older comics, we simply include the comic number in the URL, such as https://xkcd.com/2434/info.0.json. In the following code, I write a Python class that symbolizes a single xkcd comic. Given a comic number, it reaches out to the adequate URL, fetches the JSON file and transforms it into a Python dictionary. As a bonus, I write a class method that allows us to download the PNG file of the comic to a disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "direct-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xkcdComicJson:\n",
    "    \"\"\"\n",
    "    Uses the JSON interface at https://xkcd.com/ for retrieving information about a single xkcd comic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, comic_no):\n",
    "        url = f'https://xkcd.com/{comic_no}/info.0.json'\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # throws an error for bad requests\n",
    "        comic_info = response.json()  # automagically converts JSON into a dict\n",
    "        self.number = comic_no\n",
    "        self.json = comic_info\n",
    "        self.year = int(comic_info['year'])\n",
    "        self.month = int(comic_info['month'])\n",
    "        self.day = int(comic_info['day'])\n",
    "        self.title = comic_info['title']\n",
    "        self.caption = comic_info['alt']\n",
    "        self.img_url = comic_info['img']\n",
    "        self.url = url\n",
    "        self.img_name = self.img_url.split('/')[-1]\n",
    "        \n",
    "    def save_img_to_disk(self, directory='./'):\n",
    "        response = requests.get(self.img_url)\n",
    "        response.raise_for_status()\n",
    "        if directory[-1] != '/':\n",
    "            directory += '/'\n",
    "        with open(directory + f'{self.number}-{self.img_name}', mode='wb') as f:\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-radical",
   "metadata": {},
   "source": [
    "As we can see, the HTTP request is sent with the instruction `requests.get(URL)`. This returns the HTTP response. With `response.raise_for_status()`, I ask Python to raise an exception (i.e., an error) if the request was bad. In other words, if the server responded with status codes `4xx` (client error) or `5xx` (server error), we would have an error and the rest of the code would not be executed. With `response.json()` we convert the JSON file into a Python dictionary. This is straightforward, because a JSON file is nothing more than the text representation of a Python dictionary. The conversion is internally handled by the Python package [json](https://docs.python.org/3/library/json.html). The rest of the code in the ``__init__`` function is self explanatory. The method `save_img_to_disk` takes a string argument describing the path where we wish the PNG file to be saved. We use the URL of the comic image (now saved in `self`) and we download its binary representation with `requests.get`. The binary representation is directly written to disk using the `open()` method in Python.\n",
    "\n",
    "Now we can turn to using the class. Suppose that we wish to create a dataset about these comics. All we do is download all the JSON files using the `xkcdComicJson` class we wrote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sporting-offset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data download completed in 42.673 seconds.\n"
     ]
    }
   ],
   "source": [
    "df_json_rows = []\n",
    "t0 = time()\n",
    "for no in range(oldest_xkcd_comic, latest_xkcd_comic+1):\n",
    "    comic = xkcdComicJson(no)\n",
    "    df_json_rows.append(comic.json)\n",
    "    # comic.save_img_to_disk()\n",
    "t1 = time()\n",
    "time_json = t1 - t0\n",
    "df_json = pd.DataFrame(df_json_rows)\n",
    "print(\"Data download completed in {:.3f} seconds.\".format(time_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-season",
   "metadata": {},
   "source": [
    "<sup>We never grow a `pandas.DataFrame` iteratively, row by row. An accurate and detailed account on the reason is found [here](https://stackoverflow.com/a/56746204).</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "activated-south",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>num</th>\n",
       "      <th>link</th>\n",
       "      <th>year</th>\n",
       "      <th>news</th>\n",
       "      <th>safe_title</th>\n",
       "      <th>transcript</th>\n",
       "      <th>alt</th>\n",
       "      <th>img</th>\n",
       "      <th>title</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>3</td>\n",
       "      <td>2432</td>\n",
       "      <td></td>\n",
       "      <td>2021</td>\n",
       "      <td></td>\n",
       "      <td>Manage Your Preferences</td>\n",
       "      <td></td>\n",
       "      <td>Manage cookies related to essential site funct...</td>\n",
       "      <td>https://imgs.xkcd.com/comics/manage_your_prefe...</td>\n",
       "      <td>Manage Your Preferences</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>3</td>\n",
       "      <td>2433</td>\n",
       "      <td></td>\n",
       "      <td>2021</td>\n",
       "      <td></td>\n",
       "      <td>Mars Rovers</td>\n",
       "      <td></td>\n",
       "      <td>I just Googled 'roomba sojourner mod' and was ...</td>\n",
       "      <td>https://imgs.xkcd.com/comics/mars_rovers.png</td>\n",
       "      <td>Mars Rovers</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>3</td>\n",
       "      <td>2434</td>\n",
       "      <td></td>\n",
       "      <td>2021</td>\n",
       "      <td></td>\n",
       "      <td>Vaccine Guidance</td>\n",
       "      <td></td>\n",
       "      <td>I can't wait until I'm fully vaccinated and ca...</td>\n",
       "      <td>https://imgs.xkcd.com/comics/vaccine_guidance.png</td>\n",
       "      <td>Vaccine Guidance</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>3</td>\n",
       "      <td>2435</td>\n",
       "      <td></td>\n",
       "      <td>2021</td>\n",
       "      <td></td>\n",
       "      <td>Geothmetic Meandian</td>\n",
       "      <td></td>\n",
       "      <td>Pythagorean means are nice and all, but throwi...</td>\n",
       "      <td>https://imgs.xkcd.com/comics/geothmetic_meandi...</td>\n",
       "      <td>Geothmetic Meandian</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>3</td>\n",
       "      <td>2436</td>\n",
       "      <td></td>\n",
       "      <td>2021</td>\n",
       "      <td></td>\n",
       "      <td>Circles</td>\n",
       "      <td></td>\n",
       "      <td>( MSTE ( AR ) CD )</td>\n",
       "      <td>https://imgs.xkcd.com/comics/circles.png</td>\n",
       "      <td>Circles</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month   num link  year news               safe_title transcript  \\\n",
       "82     3  2432       2021       Manage Your Preferences              \n",
       "83     3  2433       2021                   Mars Rovers              \n",
       "84     3  2434       2021              Vaccine Guidance              \n",
       "85     3  2435       2021           Geothmetic Meandian              \n",
       "86     3  2436       2021                       Circles              \n",
       "\n",
       "                                                  alt  \\\n",
       "82  Manage cookies related to essential site funct...   \n",
       "83  I just Googled 'roomba sojourner mod' and was ...   \n",
       "84  I can't wait until I'm fully vaccinated and ca...   \n",
       "85  Pythagorean means are nice and all, but throwi...   \n",
       "86                                 ( MSTE ( AR ) CD )   \n",
       "\n",
       "                                                  img  \\\n",
       "82  https://imgs.xkcd.com/comics/manage_your_prefe...   \n",
       "83       https://imgs.xkcd.com/comics/mars_rovers.png   \n",
       "84  https://imgs.xkcd.com/comics/vaccine_guidance.png   \n",
       "85  https://imgs.xkcd.com/comics/geothmetic_meandi...   \n",
       "86           https://imgs.xkcd.com/comics/circles.png   \n",
       "\n",
       "                      title day  \n",
       "82  Manage Your Preferences   3  \n",
       "83              Mars Rovers   5  \n",
       "84         Vaccine Guidance   8  \n",
       "85      Geothmetic Meandian  10  \n",
       "86                  Circles  12  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-portland",
   "metadata": {},
   "source": [
    "_Voilà!_ While this example is not particularly interesting from an economic perspective, we can see how we can leverage the [requests](https://requests.readthedocs.io/) package of Python to download data. This technique can also come in handy when we need to download many files that are listed as clickable links on a website. All we need to do is to obtain the relevant URLs, include them in a Python list and run a simple `for` loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-spectrum",
   "metadata": {},
   "source": [
    "## HTML parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-motel",
   "metadata": {},
   "source": [
    "Now, suppose that xkcd did not have this nice and clean JSON interface. How can we obtain the same dataset (or an observationally equivalent one anyway) otherwise? \n",
    "\n",
    "The [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) package provides an alternative. All this package does is parse a HTML file and provide some convenience utilities to navigate it. The Quick Start section of the documentation has a simple walk-through with a simple HTML document. I will not repeat the same here. Instead, I will directly jump onto how we can use the package, hands-on. A fundamental facility we need are the Web Developer Tools of your browser. You can bring them up by pressing <kbd>F12</kbd> or <kbd>Ctrl</kbd>+<kbd>Shift</kbd>+<kbd>I</kbd>. A side pane will appear with an overwhelming bunch of information. However, all we need is the HTML inspector. This shows us the HTML code tree that structures the webpage. What we need to do before writing any Python code is carefully inspect the HTML code around our objects of interest.\n",
    "\n",
    "![inspector on xkcd](../slides/assets/xkcd-home.png)\n",
    "\n",
    "As you will see in your experience, the most popular HTML tag is `<div>`. This is a generic container, which can be filled with content and styled (e.g., colored). In well-coded websites, each `<div>` tag comes with a `id` and/or `class` attributes, which provide a (not necessarily unique) identifier. From a technical point of view, these attributes are useful for a Cascading Style Sheet (CSS). This is the file that fills colors, decides which fonts to use and so on. By carefully looking at how HTML tags and attributes are used, we may be able to find unique patterns that identify the objects of interest. In our example, we have the following.\n",
    "\n",
    "- Most of the information we need is in a `<div>` container with `id` attribute `comic`.\n",
    "- Within such container, the tag `<img>` has a `src` attribute pointing at the location of the image on the remote server.\n",
    "- In the same `<img>` tag, the attribute `title` gives, contrary to expectations, the caption (i.e., the _alt_ text) of the comic.\n",
    "- There is a `<div>` tag with `id` attribute `ctitle` contains the title of the comic.\n",
    "\n",
    "Note that with this approach we have no information on the date in which each comic was published. This is exclusively due to the fact that the HTML code does not contain this information. However, a last-modified date is provided among the header information of the HTTP response giving the image of the comic. But, such date would also reflect dates in which the comic was simply changed, as opposed to the date of first upload, so it may be subject to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "precious-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xkcdComicSoup:\n",
    "    \"\"\"\n",
    "    Uses Beautiful Soup to parse the HTML page for a given comic.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, comic_no):\n",
    "        url = f'https://xkcd.com/{comic_no}/'\n",
    "        response = requests.get(url)                     # retrieve the remote HTML document\n",
    "        response.raise_for_status()                      # make sure the request was successful\n",
    "        soup = BeautifulSoup(response.text)              # parse the HTML code\n",
    "        container = soup.find('div', id='comic')         # find a specific unique occurrence of this <div> tag\n",
    "        self.img_url = 'https:' + container.img['src']\n",
    "        self.caption = container.img['title']            # access <div>...<img src=\"...\" title=\"...\">...</div>\n",
    "        self.title = soup.find('div', id='ctitle').text  # find a specific unique occurrence of this <div> tag\n",
    "        self.number = comic_no\n",
    "        self.url = url\n",
    "        self.img_name = self.img_url.split('/')[-1]\n",
    "        self._soup = soup\n",
    "        self.img_response = requests.get(self.img_url)   # retrieve the image only\n",
    "        self.img_response.raise_for_status()             # make sure this request was successful\n",
    "        self.date = self.img_response.headers['Last-Modified']\n",
    "    \n",
    "    def save_img_to_disk(self, directory='./'):\n",
    "        if directory[-1] != '/':\n",
    "            directory += '/'\n",
    "        with open(directory + f'{self.number}-{self.img_name}', mode='wb') as f:\n",
    "            f.write(self.img_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-candy",
   "metadata": {},
   "source": [
    "Equipped with this new class, we can use it iteratively again to collect information and build a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "superior-defense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data download completed in 68.071 seconds.\n"
     ]
    }
   ],
   "source": [
    "df_soup_rows = []\n",
    "t0 = time()\n",
    "for no in range(oldest_xkcd_comic, latest_xkcd_comic+1):\n",
    "    comic = xkcdComicSoup(no)\n",
    "    row = {\n",
    "        'number':   comic.number,\n",
    "        'date':     comic.date,\n",
    "        'title':    comic.title,\n",
    "        'caption':  comic.caption,\n",
    "        'img_name': comic.img_name,\n",
    "        'img':      comic.img_url\n",
    "    }\n",
    "    df_soup_rows.append(row)\n",
    "    # comic.save_img_to_disk()\n",
    "t1 = time()\n",
    "time_soup = t1 - t0\n",
    "df_soup = pd.DataFrame(df_soup_rows)\n",
    "print(\"Data download completed in {:.3f} seconds.\".format(time_soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "square-blues",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>caption</th>\n",
       "      <th>img_name</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2432</td>\n",
       "      <td>Thu, 04 Mar 2021 00:09:10 GMT</td>\n",
       "      <td>Manage Your Preferences</td>\n",
       "      <td>Manage cookies related to essential site funct...</td>\n",
       "      <td>manage_your_preferences.png</td>\n",
       "      <td>https://imgs.xkcd.com/comics/manage_your_prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2433</td>\n",
       "      <td>Sat, 06 Mar 2021 01:47:55 GMT</td>\n",
       "      <td>Mars Rovers</td>\n",
       "      <td>I just Googled 'roomba sojourner mod' and was ...</td>\n",
       "      <td>mars_rovers.png</td>\n",
       "      <td>https://imgs.xkcd.com/comics/mars_rovers.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2434</td>\n",
       "      <td>Tue, 09 Mar 2021 03:04:34 GMT</td>\n",
       "      <td>Vaccine Guidance</td>\n",
       "      <td>I can't wait until I'm fully vaccinated and ca...</td>\n",
       "      <td>vaccine_guidance.png</td>\n",
       "      <td>https://imgs.xkcd.com/comics/vaccine_guidance.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2435</td>\n",
       "      <td>Wed, 10 Mar 2021 23:45:59 GMT</td>\n",
       "      <td>Geothmetic Meandian</td>\n",
       "      <td>Pythagorean means are nice and all, but throwi...</td>\n",
       "      <td>geothmetic_meandian.png</td>\n",
       "      <td>https://imgs.xkcd.com/comics/geothmetic_meandi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2436</td>\n",
       "      <td>Fri, 12 Mar 2021 18:20:11 GMT</td>\n",
       "      <td>Circles</td>\n",
       "      <td>( MSTE ( AR ) CD )</td>\n",
       "      <td>circles.png</td>\n",
       "      <td>https://imgs.xkcd.com/comics/circles.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number                           date                    title  \\\n",
       "82    2432  Thu, 04 Mar 2021 00:09:10 GMT  Manage Your Preferences   \n",
       "83    2433  Sat, 06 Mar 2021 01:47:55 GMT              Mars Rovers   \n",
       "84    2434  Tue, 09 Mar 2021 03:04:34 GMT         Vaccine Guidance   \n",
       "85    2435  Wed, 10 Mar 2021 23:45:59 GMT      Geothmetic Meandian   \n",
       "86    2436  Fri, 12 Mar 2021 18:20:11 GMT                  Circles   \n",
       "\n",
       "                                              caption  \\\n",
       "82  Manage cookies related to essential site funct...   \n",
       "83  I just Googled 'roomba sojourner mod' and was ...   \n",
       "84  I can't wait until I'm fully vaccinated and ca...   \n",
       "85  Pythagorean means are nice and all, but throwi...   \n",
       "86                                 ( MSTE ( AR ) CD )   \n",
       "\n",
       "                       img_name  \\\n",
       "82  manage_your_preferences.png   \n",
       "83              mars_rovers.png   \n",
       "84         vaccine_guidance.png   \n",
       "85      geothmetic_meandian.png   \n",
       "86                  circles.png   \n",
       "\n",
       "                                                  img  \n",
       "82  https://imgs.xkcd.com/comics/manage_your_prefe...  \n",
       "83       https://imgs.xkcd.com/comics/mars_rovers.png  \n",
       "84  https://imgs.xkcd.com/comics/vaccine_guidance.png  \n",
       "85  https://imgs.xkcd.com/comics/geothmetic_meandi...  \n",
       "86           https://imgs.xkcd.com/comics/circles.png  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_soup.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-mediterranean",
   "metadata": {},
   "source": [
    "Here it is. The dataset is obviously not identical to the previous one we obtained, but it gets really close. We should notice, however, that the column `date` is indeed not the date the comic was published on. It is the date on which the image of the comic was last modified. It looks like some comics have been updated after they have been published, as well as some comics were uploaded before going public.\n",
    "\n",
    "Note that this method took much longer than the previous one. This is due to the fact that, with BeautifulSoup, we need to download a full webpage for every comic, which in itself contains the image. Additionally, we re-download the image separately exclusively to access the \"Last-Modified\" header that pertains that specific file. With the JSON interface, we only needed to download very small text files. Obviously, in economically relevant applications, we rarely get to choose which method to use. However, it is important here to understand that web scraping tends to take its time, however efficient our code may be. This download time also critically depends on our Internet connection. For best results in terms of time and stability, it is recommended to use a cabled connection over a wireless one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-dollar",
   "metadata": {},
   "source": [
    "## Browser Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-polyester",
   "metadata": {},
   "source": [
    "Suppose that, for some reason, neither of the previous techniques were viable. This may well happen when the content of the page dynamically changes. In other words, suppose that the website loads new content in the webpage without reloading the whole page. This is often the case with websites that support \"infinite scrolling\": a technique to automatically load new content, to be stacked vertically, as long as the user scrolls down (e.g., Twitter, Facebook, reddit). The last resort option in these cases is to automate your own browser, such that you can emulate human behavior using a program.\n",
    "\n",
    "Browser webdrivers are essential tools in the toolkit of a web developer. They are mostly used for testing websites and trying many possible combinations of user behavior, without actually doing it manually. We can use webdrivers to our advantage. Instead of testing a website, we would just be instructing our browser to navigate to a given webpage, move around and select or read elements using HTML identifiers. In essence, our practical approach is unchanged relative to HTML parsing: we still need to manually inspect the HTML code of the webpage. However, the main difference is that we write code that walks our same steps, just automatically. [Selenium](https://www.selenium.dev/) is a set of code bindings to a number of programming languages, including Python.\n",
    "\n",
    "Using a webdriver is relatively simple, but it requires an additional piece of software. Specifically, [such software is the webdriver itself](https://www.selenium.dev/selenium/docs/api/py/index.html#drivers)! This is a small executable file that provides the necessary code to translate commands in your programming language into human-like actions in the browser. We have two options here. Either (1) we download the executable, place it in an arbitrary folder, and instruct Selenium about such folder, or (2) we place the executable inside a folder known listed in the [PATH variable](https://en.wikipedia.org/wiki/PATH_(variable)) of the Operating System. I opted for the first option, purely out of personal preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fuzzy-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox(executable_path='C:/Users/Andrea/Documents/geckodriver.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-paradise",
   "metadata": {},
   "source": [
    "This line creates a new browser window that is automatically controlled by the code we will write next.\n",
    "\n",
    "To showcase the power of browser automation, we take an approach to searching comics different from the one we used above. We start from the latest comic, found at https://xkcd.com, and then we iteratively press the \"Previous\" button up until we get to comic number 2350. This shows how we can emulate human behavior, by means of clicks. Therefore, we will have a `while` loop that uses Selenium functions to find elements. To find these elements, we use the following strategies (in bold, the information we gather):\n",
    "\n",
    "- The **title of the comic** is in a `<div>` tag with `id` attribute `ctitle`\n",
    "- The `<div>` tag with `id` attribute `middleContainer` contains the following lines\n",
    "  - A line with the comic title\n",
    "  - A line with navigation buttons (above the comic)\n",
    "  - A line with navigation buttons (below the comic)\n",
    "  - An empty line\n",
    "  - A line with a \"permalink\" to the comic, which gives us the **comic number**\n",
    "  - A line with a \"hotlink\" to the comic image, which gives us the URL of the comic image\n",
    "- The `<div>` tag with `id` attribute `comic` contains a `<img>` tag that contains\n",
    "  - The **URL of the comic image**, at the `src` attribute\n",
    "  - The **caption of the comic**, at the `title` attribute\n",
    "- The \"Previous\" button is a link with text `< Prev`\n",
    "\n",
    "We close the procedure by closing the automated browser window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "modern-flesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data download completed in 45.347 seconds.\n"
     ]
    }
   ],
   "source": [
    "df_dom_rows = []\n",
    "t0 = time()\n",
    "browser.get('https://xkcd.com')  # point the browser to the homepage\n",
    "number = 3000\n",
    "\n",
    "while number > oldest_xkcd_comic:\n",
    "    # Find the number of the comic\n",
    "    div = browser.find_element_by_xpath('//div[@id=\"middleContainer\"]')\n",
    "    lines = div.text.split('\\n')\n",
    "    permalink = lines[-2].split(': ')[-1]\n",
    "    number = int( permalink.split('/')[-2] )\n",
    "    # img_url = lines[-1].split(': ')[-1]  we could use this instead of ...\n",
    "    \n",
    "    # Find the title of the comic\n",
    "    title = browser.find_element_by_xpath('//div[@id=\"ctitle\"]').text\n",
    "    \n",
    "    # Find the caption of the comic\n",
    "    try:\n",
    "        img = browser.find_element_by_xpath('//div[@id=\"comic\"]/img')\n",
    "    except NoSuchElementException:  # see 2399\n",
    "        img = browser.find_element_by_xpath('//div[@id=\"comic\"]/a/img')\n",
    "    caption = img.get_attribute('title')\n",
    "    \n",
    "    # Find the URL of the comic image\n",
    "    img_url = img.get_attribute('src')  # ... this\n",
    "    \n",
    "    # Find the name of the PNG file\n",
    "    img_name = img_url.split('/')[-1]\n",
    "    \n",
    "    # Collect information for dataset\n",
    "    row = {\n",
    "        'number': number,\n",
    "        'title': title,\n",
    "        'caption': caption,\n",
    "        'img_name': img_name,\n",
    "        'img': img_url\n",
    "    }\n",
    "    \n",
    "    # Append information to list\n",
    "    df_dom_rows.append(row)\n",
    "    \n",
    "    # Go to the previous comic\n",
    "    browser.find_element_by_link_text(\"< Prev\").click()\n",
    "    \n",
    "browser.quit()  # close the automated browser window\n",
    "t1 = time()\n",
    "time_dom = t1-t0\n",
    "print(\"Data download completed in {:.3f} seconds.\".format(time_dom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "distinguished-privacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>caption</th>\n",
       "      <th>img_name</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2436</td>\n",
       "      <td>Circles</td>\n",
       "      <td>( MSTE ( AR ) CD )</td>\n",
       "      <td>circles.png</td>\n",
       "      <td>https://imgs.xkcd.com/comics/circles.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2435</td>\n",
       "      <td>Geothmetic Meandian</td>\n",
       "      <td>Pythagorean means are nice and all, but throwi...</td>\n",
       "      <td>geothmetic_meandian.png</td>\n",
       "      <td>https://imgs.xkcd.com/comics/geothmetic_meandi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2434</td>\n",
       "      <td>Vaccine Guidance</td>\n",
       "      <td>I can't wait until I'm fully vaccinated and ca...</td>\n",
       "      <td>vaccine_guidance.png</td>\n",
       "      <td>https://imgs.xkcd.com/comics/vaccine_guidance.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2433</td>\n",
       "      <td>Mars Rovers</td>\n",
       "      <td>I just Googled 'roomba sojourner mod' and was ...</td>\n",
       "      <td>mars_rovers.png</td>\n",
       "      <td>https://imgs.xkcd.com/comics/mars_rovers.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2432</td>\n",
       "      <td>Manage Your Preferences</td>\n",
       "      <td>Manage cookies related to essential site funct...</td>\n",
       "      <td>manage_your_preferences.png</td>\n",
       "      <td>https://imgs.xkcd.com/comics/manage_your_prefe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number                    title  \\\n",
       "0    2436                  Circles   \n",
       "1    2435      Geothmetic Meandian   \n",
       "2    2434         Vaccine Guidance   \n",
       "3    2433              Mars Rovers   \n",
       "4    2432  Manage Your Preferences   \n",
       "\n",
       "                                             caption  \\\n",
       "0                                 ( MSTE ( AR ) CD )   \n",
       "1  Pythagorean means are nice and all, but throwi...   \n",
       "2  I can't wait until I'm fully vaccinated and ca...   \n",
       "3  I just Googled 'roomba sojourner mod' and was ...   \n",
       "4  Manage cookies related to essential site funct...   \n",
       "\n",
       "                      img_name  \\\n",
       "0                  circles.png   \n",
       "1      geothmetic_meandian.png   \n",
       "2         vaccine_guidance.png   \n",
       "3              mars_rovers.png   \n",
       "4  manage_your_preferences.png   \n",
       "\n",
       "                                                 img  \n",
       "0           https://imgs.xkcd.com/comics/circles.png  \n",
       "1  https://imgs.xkcd.com/comics/geothmetic_meandi...  \n",
       "2  https://imgs.xkcd.com/comics/vaccine_guidance.png  \n",
       "3       https://imgs.xkcd.com/comics/mars_rovers.png  \n",
       "4  https://imgs.xkcd.com/comics/manage_your_prefe...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dom = pd.DataFrame(df_dom_rows)\n",
    "df_dom.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-beatles",
   "metadata": {},
   "source": [
    "Overall, these procedures retrieve roughly the same data. The main differences pertain the method and the execution times. While browser automation may seem more intuitive, as it directly relates to what we normally do in a browser, it is also the method that takes more time. Furthermore, if the connection to the Internet is not particularly fast, HTTP programming may just be the best option, because it allows for optimizations over what exactly to download. However, when scraping data, we often do not have a choice. Only one method is feasible. Knowing what to expect is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "regional-symphony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comics retrieved: 86.\n",
      "HTTP programming took   42.673 seconds.\n",
      "HTML parsing took       68.071 seconds.\n",
      "Browser automation took 45.347 seconds.\n"
     ]
    }
   ],
   "source": [
    "print('Comics retrieved: {:0d}.'.format(latest_xkcd_comic - oldest_xkcd_comic))\n",
    "print('HTTP programming took   {:.3f} seconds.'.format(time_json))\n",
    "print('HTML parsing took       {:.3f} seconds.'.format(time_soup))\n",
    "print('Browser automation took {:.3f} seconds.'.format(time_dom))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
